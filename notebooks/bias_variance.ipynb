{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfebe3-ed79-4316-a34b-9473b4965c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes, fetch_openml,load_iris,fetch_california_housing\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression, RFE, SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "RepeatedStratifiedKFold, \n",
    "cross_val_score, \n",
    "train_test_split, \n",
    "GridSearchCV,\n",
    "cross_val_predict, \n",
    "learning_curve, \n",
    "validation_curve)\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error,zero_one_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "#sharper plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV,\n",
    "                                  SGDClassifier)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1575dd2-d694-4aa1-9a09-cfb03d07c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/data/machine_learning/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc4ff3-053b-492b-84ed-4943a7050863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_err(params,fun_cv, label):\n",
    "    mus = np.mean(fun_cv, axis=-1)\n",
    "    stds = np.std(fun_cv, axis=-1)\n",
    "    plt.plot(params, mus, label=label)\n",
    "    plt.fill_between(params, mus - stds, mus + stds, alpha=0.2)\n",
    "    #plt.fill_between(params, mus - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "    # plt.errorbar(params, mus, stds, linestyle='None', marker='^', label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba868ed5-7391-405c-b1df-863448b00859",
   "metadata": {},
   "source": [
    "Мы уже знаем, что среднеквадратичный риск на фиксированной выборке X можно расписать как \n",
    "$$E = Var(h) + Bias^2(h) + Noise(y)$$\n",
    "Здесь $Bias^2(h) = E_x[(\\overline{h}(X) - \\overline{y}(X))^2]$ показывает, насколько средняя модель отклонится от матожидания таргета (идеальной модели). \n",
    "$Var(h) = E_{x,D}[(h(X, D) - \\overline{h}(X))^2]$ - показывает разброс обученных моделей относительно среднего ответа. \n",
    "$Noise(y) = E_{x,y}[(\\overline{y}(X) - Y)^2]$ - дисперсия самого таргета при фиксированном x. Это неустранимая ошибка, которой соответствует самый идеальный прогноз.\n",
    "\n",
    "Смещение показывает, насколько хорошо можно с помощью данного семейства моделей приблизиться к оптимальной модели. Как правило, оно маленькое у сложных семейств и большое у относительно простых. Вопрос: Назовите такие семейства.\n",
    "\n",
    "Дисперсия показывает, насколько будет изменяться предсказание в зависимости от выборки - то есть насколько ваше семейство склонно к переобучению. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c2085-b44f-4672-b646-34612fcfb70b",
   "metadata": {},
   "source": [
    "Построим для иллюстрации простой пример и проверим, соответствует ли утверждение, что при увеличении сложности модели уменьшится смещение. Также проверим, растет ли при увеличении сложности модели дисперсия. Для начала посмотрим на простую зависимость, довольно сильно зашумленную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88be1c-3bfe-41cb-a1ab-866f918ba58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2.5 * np.cos(1.5 * x)*x  + 1 * x\n",
    "    \n",
    "def generate_data(n_samples: int = 50, noise: float = 3, n_noise_samples: int = 1):\n",
    "    x = np.random.rand(n_samples) * 20 - 10\n",
    "    x = np.sort(x)\n",
    "\n",
    "    y = np.zeros((n_samples, n_noise_samples))\n",
    "    for i in range(n_noise_samples):\n",
    "        y[:, i] = f(x) + np.random.normal(0.0, noise, n_samples)\n",
    "    return x.reshape((n_samples, 1)), y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef2dc2-03ab-43a9-b1ec-475a385b721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_ticks = np.linspace(-10, 10)\n",
    "X, y = generate_data(noise=4)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "plt.title('Data sample', fontsize=15)\n",
    "plt.plot(x_ticks, f(x_ticks), color=\"blue\", label=\"True function\")\n",
    "plt.scatter(x_train, y_train, color=\"green\", label=\"Train samples\")\n",
    "plt.scatter(x_test, y_test, color=\"magenta\", label=\"Test samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e2511-0eb5-4547-adc8-a4b4757fd834",
   "metadata": {},
   "source": [
    "Теперь мы можем сгенерировать датасеты и попробовать обучать модели на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d0326-dbae-4b88-b99e-af39d6cf5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_subset(model_class, parameters, n_train_samples: int = 20, noise: float = 3):\n",
    "    x_train, y_train = generate_data(n_samples=n_train_samples, noise=noise)\n",
    "    model = model_class(**parameters)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model, x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c69509-8cfd-4e64-ae7a-5fb71071e374",
   "metadata": {},
   "source": [
    "Для начала посмотрим, как вообще будет выглядеть предсказание в зависимости от глубины дерева. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460e886-fff5-4da1-b1fb-4bd14444178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": 1}\n",
    "np.random.seed(1243)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12,4), dpi=150, sharey=True)\n",
    "fig.suptitle(\"Decision trees on different datasets\", fontsize=15)\n",
    "for i in range(3):\n",
    "    model, x_train, y_train = train_on_subset(model_class=DecisionTreeRegressor, parameters=parameters, noise=3)\n",
    "    preds = model.predict(x_ticks[:, None])\n",
    "    ax[i].plot(x_ticks, f(x_ticks), color=\"blue\", label=\"True function\", linestyle=\"--\")\n",
    "    ax[i].scatter(x_train, y_train, color=\"green\", label=\"Train samples\")\n",
    "    ax[i].plot(x_ticks, preds, color=\"magenta\", label=\"Prediction\")\n",
    "    ax[i].set_title(f\"sample {i}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685c5c3",
   "metadata": {},
   "source": [
    "**Задание:** Постройте графики для большей глубины. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd368a71-958e-4b71-b3bd-e32e4ef56c7e",
   "metadata": {},
   "source": [
    "Теперь мы можем получить оценки ошибки и ее составляющих. Для этого насемплируем 1000 выборок, предварительно выделив общий тест сет, и вычислим test MSE для каждой из выборок."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38727eed",
   "metadata": {},
   "source": [
    "**Задание:** Дополните код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5c8ef-cea0-4cff-98d3-09bf06f9e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(\n",
    "    model_cards,\n",
    "    n_repeats: int, noise: float, n_train_samples: int, \n",
    "    n_test_samples: int, n_noise_samples: int):\n",
    "  \n",
    "    x_test, y_test = generate_data(n_samples=n_test_samples, \n",
    "                                 n_noise_samples=n_noise_samples, noise=noise)\n",
    "    predictions = {} # to avoid creation of [] \n",
    "    results = []\n",
    "    \n",
    "    for i, (model_class, parameters) in enumerate(model_cards):\n",
    "        np.random.seed(12341)\n",
    "        predictions[i] = [] \n",
    "        for j in tqdm(range(n_repeats), desc=f\"{model_class.__name__}, {parameters}\"):\n",
    "            #  get new predictions on new subset and append to predictions of the model\n",
    "             \n",
    "        results.append({\n",
    "          \"name\": model_class.__name__,\n",
    "          \"parameters\": parameters,\n",
    "          \"predictions\": np.stack(predictions[i]),\n",
    "          \"x_test\": x_test,\n",
    "          \"y_test\": y_test\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28c2e5-ec19-40a3-ae6e-575c9279f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[DecisionTreeRegressor, {\"max_depth\": depth}] for depth in range(1, 16)]\n",
    "\n",
    "results = get_predictions(\n",
    "    models,\n",
    "    n_repeats=1000, \n",
    "    n_test_samples=500, \n",
    "    n_train_samples=500,\n",
    "    n_noise_samples=300,\n",
    "    noise=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f89bc-c76d-4e2a-b11e-679f7170b11d",
   "metadata": {},
   "source": [
    "**Задание**: Посчитайте размеры массивов предсказаний, x_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba485d85-fdcf-4e11-a831-d429fa05a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"predictions shape: n_repeats x n_test_samples = {results[0]['predictions'].shape}\")\n",
    "print(f\"x_test shape: n_test_samples x 1 = {results[0]['x_test'].shape}\")\n",
    "print(f\"y_test shape: n_test_samples x n_noise_samples = {results[0]['y_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddde72",
   "metadata": {},
   "source": [
    "**Задание**: Дополните код, вычисляющий bias, variance, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9de9e3-0031-4b95-ac17-4814c281d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_variance(results) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for res in results:\n",
    "        x_test, y_test = res[\"x_test\"], res[\"y_test\"]\n",
    "        predictions = res[\"predictions\"]\n",
    "        bias = # calculate bias\n",
    "        variance = # calculate variance\n",
    "        noise = # calculate noise\n",
    "        error = (predictions[..., None] - y_test[None]) ** 2\n",
    "\n",
    "        records.append({\n",
    "            \"name\": res[\"name\"],\n",
    "            \"parameters\": res[\"parameters\"],\n",
    "            \"bias_sq\": np.mean(bias ** 2),\n",
    "            \"variance\": np.mean(variance),\n",
    "            \"noise\": np.mean(noise),\n",
    "            \"mse\": np.mean(error),\n",
    "            \"error_decomposed\": np.mean(bias ** 2 + variance + noise)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c832d97-47c2-4ae7-9214-0fadccb20d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_trees = get_bias_variance(results)\n",
    "bias_variance_trees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b48e4-d8e1-47c0-9307-7f6217c03a4e",
   "metadata": {},
   "source": [
    "Теперь мы можем построить график разложения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16ab0a-3a58-4b42-b7d9-961f998d6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance(\n",
    "    bias_variance_results: pd.DataFrame, \n",
    "    parameter_name: str,  \n",
    "    parameter_values: list[float]\n",
    ") -> None:\n",
    "    plt.figure(figsize=(8, 5), dpi=150)\n",
    "    plt.xticks(parameter_values)\n",
    "    plt.plot(parameter_values, bias_variance_results.bias_sq, label=\"bias\", color=\"blue\")\n",
    "    plt.plot(parameter_values, bias_variance_results.variance, label=\"variance\", color=\"orange\")\n",
    "    plt.plot(parameter_values, bias_variance_results.noise, label=\"noise\", color=\"green\")\n",
    "    plt.plot(parameter_values, bias_variance_results.mse, label=\"MSE\", color=\"magenta\")\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.legend(fontsize=10, loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a522c-9584-4e8c-90be-dac96367ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = bias_variance_trees.apply(lambda r: r.parameters[\"max_depth\"], axis=1)\n",
    "\n",
    "plot_bias_variance(bias_variance_trees, parameter_name=\"Tree depth\",  parameter_values=depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983ba6f-ac00-4b28-8b71-9efb0adc8bbd",
   "metadata": {},
   "source": [
    "Видно, что при увеличении глубины деревьев смещение падает практически до нуля, а разброс нарастает. Проверим, будет ли это сохраняться и дальше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbd89c-3eb2-43a4-9fc0-ef8e89891f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [[DecisionTreeRegressor, {\"max_depth\": depth}] for depth in range(1, 26)]\n",
    "\n",
    "results = get_predictions(\n",
    "    models,\n",
    "    n_repeats=1000, \n",
    "    n_test_samples=500, \n",
    "    n_train_samples=500,\n",
    "    n_noise_samples=300,\n",
    "    noise=3\n",
    ")\n",
    "bias_variance_many_trees = get_bias_variance(results)\n",
    "plot_bias_variance(\n",
    "    bias_variance_many_trees, \n",
    "    parameter_name=\"Tree depth\",  \n",
    "    parameter_values=bias_variance_many_trees.apply(lambda r: r.parameters[\"max_depth\"], axis=1)\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f188c4-0ece-4d5d-87b6-0d51bc83b28d",
   "metadata": {},
   "source": [
    "Вопрос: Почему с большой глубиной дерева значения ошибки и разложения перестают меняться? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22e7c7-be0e-4e6d-bcbd-245e04314b15",
   "metadata": {},
   "source": [
    "**Задание**: Постройте зависимости для других сочетаний таргета и шума.\n",
    "\n",
    "**Задание**: Постройте графики предсказаний всех 1000 деревьев на одном графике (и таргета тоже)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4785f2-5ac3-4d5d-a7e5-5b994cd6fbeb",
   "metadata": {},
   "source": [
    "Перейдем к реальному примеру. Для таких данных, конечно, тоже можно посчитать, как раскладывается ошибка."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3edb9-673e-4a7f-aec6-dbe31f75fc33",
   "metadata": {},
   "source": [
    "Рассмотрим разделение ошибки на примере данных о продаже зданий в Калифорнии. Разложение для некоторых функций, в том числе, для mse, поддерживается библиотекой mlextend. Сейчас попробуем ответить на два вопроса: действительно ли в реальной жизни ошибка подчиняется этому свойству и как изменение модели повлияет на разложение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e989e-842e-4dbd-856a-b1e926205d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# Model definition\n",
    "model = LinearRegression()\n",
    "# Estimation of bias and variance using bias_variance_decomp\n",
    "#Note here we are using loss as 'mse' and setting default bootstrap num_rounds to 200\n",
    "mse, bias, var = bias_variance_decomp(\n",
    "    model, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=123\n",
    "    )\n",
    "y_pred=model.predict(X_test)\n",
    "# summarize results\n",
    "print('MSE from bias_variance lib [avg expected loss]: %.3f' % mse)\n",
    "print('Avg Bias: %.3f' % bias)\n",
    "print('Avg Variance: %.3f' % var)\n",
    "print('Mean Square error by Sckit-learn lib: %.3f' % mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eb3de",
   "metadata": {},
   "source": [
    "**Задание(*):** Реализуйте кастомную функцию для bias-variance-decomp, сравните с версией из mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d594a9-24ff-4b95-93a2-fe018c1bf8ee",
   "metadata": {},
   "source": [
    "В результате наших расчетов из приведенного выше очевидно, что общая ошибка = смещение + дисперсия, мы также могли видеть, что MSE, рассчитанная на основе библиотеки sckit, почти равна той, что мы получили их mlextend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8380db9-9e58-406c-9d1f-5f0d29b51bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.05)\n",
    "error_reg_las, bias_reg_las, var_reg_las = bias_variance_decomp(\n",
    "    lasso_model, X_train, y_train, X_test, y_test, loss='mse', random_seed=123\n",
    "    )\n",
    "\n",
    "y_pred=lasso_model.predict(X_test)\n",
    "print('MSE from bias_variance lib [avg expected loss]: %.3f' % error_reg_las)\n",
    "print('Avg Bias: %.3f' % bias_reg_las)\n",
    "print('Avg Variance: %.3f' % var_reg_las)\n",
    "print('Mean Square error by Sckit-learn lib: %.3f' % mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d296aa-ae64-4b68-926d-eb807cb216b8",
   "metadata": {},
   "source": [
    "Можно заметить, что после регуляризации смещение увеличелось, дисперсия немного уменьшилась, а общая средняя ошибка также снизилась."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68468a6-855e-4747-a942-100ed7678201",
   "metadata": {},
   "source": [
    "**Задание:** Постройте график разложения для разных уровней регуляризации. При каком уровне регуляризации ошибка начинает увеличиваться из-за смещения?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445baa8-6629-40ae-84c6-052d1c1d1503",
   "metadata": {},
   "source": [
    "Как же измеряются Bias и Variance? Конечно, мы не можем оценить bias и variance без доступа ко всем возможным выборкам, но можем приблизиться к решению задачи как можно ближе. Это можно сделать с помощью бутстрапирования выборки.\n",
    "\n",
    "Бутстрап работает на удивление просто.\n",
    "Предположим, что наша выборка D размера n на представляет генеральную совокупность. После этого мы можем сгененрировать эмпирическое распределение необходимой статистики, выбирая с замещением $N >> 100$ подвыборок объема n из этой совокупности (назовем псевдовыборками), и рассчитывая для них нужную статистику. \n",
    "\n",
    "![calib_1](../additional_materials/images/bootstrap.png)\n",
    "Вообще, этот метод очень хорош для получения интервальных оценок, стандартных отклонений и прочего, даже если не задавать ограничения на распределения. \n",
    "\n",
    "Обратите внимание:\n",
    "1) Чтобы оценка была несмещённой, необходимо генерировать выборки такого же размера, как и размер исходной выборки;\n",
    "2) Количество итераций бутстрепа рекомендуется брать в диапазоне от 1000 до 10000. Этого, как правило, хватает для получения достаточно точных результатов.\n",
    "\n",
    "Если бутстрап такой замечательный, то почему его не используют во всех задачах? Основной недостаток – его скорость работы. Для больших объемов данных вычисления могут требовать знчительных временных затрат. Во вторых, если в данных присутвтуют зависимости (выборки получаются не iid), то оценка не будет приближать исходное распределение. Так что разные особенности данных тоже надо учитывать. И последнее - если исходная выборка нерепрезентативна, то и результат будет не очень.\n",
    "\n",
    "Вопрос: какие применения бутстрапа вы знаете?\n",
    "\n",
    "Вопрос: как бутстрапом получить bias и variance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a79a5-ddba-4ac8-ada0-04d1b31e0a6c",
   "metadata": {},
   "source": [
    "**Задание(*)**: Реализуйте функцию для вычисления смещения и дисперсии с помощью бутстрепа для MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c53dc5-05ca-4e10-822b-2e81853e8b9f",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048a851-49dc-457d-a3a3-7c3de7209e51",
   "metadata": {},
   "source": [
    "Существуют обобщения разложения. Последуем нотации из статьи В общем случае \"главные\" предсказания - это значения, которые отличаются наименее (по отношению к функции потерь $L$) от всех меток в Y: $y_{main} = argmin_{\\hat{y}}(E(L(y, \\hat y))$. В случае MSE это среднее, MAE - медиана. Для 0-1 лосса это мода. \n",
    "Тогда Bias и Variance можно определить как: Bias - это потери среднего предсказания по отношению к главному: $L(y, y_{main})$. Var - Средние потери предсказаний относительно среднего: $E(L(\\hat{y}, y_{main})$. \n",
    "Тогда в общем случае разложение будет выглядеть как: \n",
    "$$E = с_1Var(h) + Bias(h) + с_2Noise(y),$$\n",
    "где $с_1, c_2$ - константы, зависящие от лосса [[источник](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d94301-1942-48ae-8a20-8d1f3cd86139",
   "metadata": {},
   "source": [
    "Рассмотрим 0–1 loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{01}(\\hat{y}, y) = \\frac{1}{n}\\sum_{i=1}^n \\delta_{\\hat{y_i} \\neq y_i} \\quad \\text{with} \\quad \\delta_{\\hat{y_i} = y_i} = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } \\hat{y_i} \\neq y_{main} \\\\\n",
    "1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Этот лосс не что иное как error rate. Он очень простой, но при этом не используется на практике. Вопрос: почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf062ce-420b-43c8-9e65-c38a45e747b5",
   "metadata": {},
   "source": [
    "Смещение и дисперсия для потерь 0–1 следующие. \n",
    "\n",
    "Смещение равно 1, если основной прогноз не согласуется с истинной меткой y, и 0 в противном случае:\n",
    "$$bias_i =  \\begin{cases} \n",
    "1, & y ≠ y_{main} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Усреднение даст финальный ожидаемый $Bias$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834fd90-2377-4865-83f4-faa58d6400dc",
   "metadata": {},
   "source": [
    "Теперь к дисперсии. В случае 0-1 $L(y, \\hat y)= P(y \\neq \\hat y) = Bias + Var$.\n",
    "\n",
    "Рассмотрим случай, когда Bias=0. Тогда $Loss = Var$.\n",
    "Тогда, если подставить  $\\mathcal{L}_{01}$ в формулы для обобщенного лосса, мы получим, что дисперсия определяется как вероятность того, что предсказанная метка не соответствует главному предсказанию:\n",
    "$Var = P(\\hat y \\neq E[\\hat y]) = P(\\hat y \\neq y_{main}])$\n",
    "\n",
    "Теперь более сложный случай - когда Bias = 1. \n",
    "Тогда $L(y, \\hat y)= P(y \\neq \\hat y) = 1 - P(y = \\hat y)$. Т.к. мы рассматриваем случай $\\hat{y} \\neq y_{main}$. то получаем, что  $\\hat y \\neq y_{main}$, и выражение для лосса можно переписать как $L(y, \\hat y)=  1 - P(y \\neq y_{main}) = Bias - Var$. \n",
    "\n",
    "Получается, что увеличение Var может улучшить нашу модель! Неинтуитивно. \n",
    "Вопрос: ПОчему увеличение Var может помочь?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09840a-a604-4db5-a172-f75efcea7797",
   "metadata": {},
   "source": [
    "В качестве примера рассмотрим датасет rice classification. Этот датасет посвящен классификации риса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e745650-8d77-4d58-8294-d7518935a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + \"wdbc/data.csv\")\n",
    "data = data.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "data['diagnosis'] = data['diagnosis'].replace({'B': 0, 'M': 1}).astype(int)\n",
    "y = data[\"diagnosis\"].astype(\"int\").values\n",
    "X = data.drop(\"diagnosis\", axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee1123-f28b-4d95-950b-c9710fbb92a2",
   "metadata": {},
   "source": [
    "## Деревья решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4aa228-81f1-409d-bf19-b85aedd4e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc662523-52d7-4304-ba57-e46ae43a1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\n",
    "                \"tree\",\n",
    "                DecisionTreeClassifier(random_state=123, max_depth=6),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "clf_dt = DecisionTreeClassifier(random_state=123)\n",
    "#clf_dt.fit(X_train,y_train)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "y_pred=clf_dt.predict(X_test)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        clf_dt, X_train, y_train, X_test, y_test,\n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))\n",
    "print('Sklearn accuracy: %.3f' % clf_dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a62d5-4083-40b3-91c7-af75a0d97d3e",
   "metadata": {},
   "source": [
    "Запруним наше дерево, уменьшив его сложность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54408b33-2af2-4000-a349-204108244eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prune tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f216687-4f2c-4139-a5fa-600f2d7669e2",
   "metadata": {},
   "source": [
    "Теперь мы можем и посмотреть, как меняются составляющие с усложнениесм дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d42ed8-7f0a-4b1c-af8f-23551af31cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias, var, error= [], [], []\n",
    "max_d = 10\n",
    "for d in range(1,max_d):\n",
    "    # collect b,v,e\n",
    "#plot resulting curves\n",
    "\n",
    "plt.xlabel('Algorithm Complexity(depth)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c3606",
   "metadata": {},
   "source": [
    "Мы можем рассмотреть и увеличение степени полинома. Но это может быть очень долго!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2755ba2-e72e-4a92-a2ed-8fb6a831ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias, var, error= [], [], []\n",
    "max_d = 4\n",
    "for d in range(1, max_d):\n",
    "        # collect b,v,e\n",
    "#plot resulting curves\n",
    "\n",
    "plt.xlabel('Algorithm Complexity(degree of polynomial)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673938b-7c8c-4df5-8de6-a00d9f8dd63a",
   "metadata": {},
   "source": [
    "Выше мы видим, что общиая ожидаемая ошибка = сумма смещения + дисперсии и прунинг имеет некоторый эффект на уменьшение дисперсии. Мы также видим момент переобучения - когда bias доходит до нуля, а variance начинает повышаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e8dbe-c3da-4f22-bed0-26f829b81113",
   "metadata": {},
   "source": [
    "Случайный лес, в том числе за счет бутстрапа, позволяет уменьшить ожидаемую ошибку. В его основе композиция деревьев, обученных на случайных подвыборках с помощью беггинга, при этом при каждом разбиении случайно выбирается подмножество из всех признаков. \n",
    "При беггинге (Bootstrap Aggregating) многочисленные повторы исходного набора данных создаются с использованием случайного выбора с заменой. Каждый производный набор данных затем используется для построения новой модели, и модели собираются в ансамбль. Чтобы сделать прогноз, все модели в ансамбле опрашиваются и их результаты усредняются.\n",
    "\n",
    "В целом, беггинг (bootstrap aggregation) не увеличивает смещенеие модели, но при этом *всегда* уменьшает дисперсию. Интересно: В идеальном случае для MSE, обучение N нескоррелированных алгоритмов с помощью бутстрапа уменьшает ошибку в N раз! \n",
    "\n",
    "В своём блоге Лео Бриман (Leo Breiman), создатель случайного леса, написал следующее:\n",
    " > Random forest does not overfit. You can run as many trees as you want.\n",
    "\n",
    "Это относилось только к числу деревьев. В целом, мы можем ожидать, что дисперсия перестанет расти в какой-то момент. \n",
    "Проверим, так ли это. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb321d-923d-43da-b073-745f5f5904a9",
   "metadata": {},
   "source": [
    "**Задание**: Выведите отношение ожидаемой ошибки к ошибке одной модели в случае усреднения N моделей для MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2b752-22bd-4171-85ee-b77044f7019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_RF = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "#clf_RF.fit(X_train,y_train)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\n",
    "                \"tree\",\n",
    "                RandomForestClassifier(max_depth=5, random_state=0),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred=pipeline.predict(X_test)\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        pipeline, X_train, y_train, X_test, y_test,\n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29bd79",
   "metadata": {},
   "source": [
    "**Задание**: Постройте графики зависимости от глубины дерева для фиксированной степени полинома."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4588a92-7218-47ff-a260-130866b9a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias, var, error= [], [], []\n",
    "max_d = 10\n",
    "# Your code\n",
    "plt.xlabel('Algorithm Complexity(depth)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f3cf1",
   "metadata": {},
   "source": [
    "**Задание**: Постройте графики зависимости от числа деревьев для фиксированной степени полинома.\n",
    "\n",
    "**Задание(*):** Постройте графики зависимости от числа деревьев, использующих только k случайных фичей, где k корень от числа всех фичей, для фиксированной степени полинома.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73b44-ef10-44e1-9db9-be2ca53c8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias, var, error= [], [], []\n",
    "# Your code\n",
    "plt.xlabel('Algorithm Complexity(n_trees)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a58102-9d5d-49b3-a393-6a9f3a2b5738",
   "metadata": {},
   "source": [
    " На практике единственным ограничением размера леса является время вычислений, поскольку можно обучить бесконечное количество деревьев без увеличения систематической ошибки и с постоянным (хотя и асимптотически уменьшающимся) уменьшением дисперсии.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c8c5e-09fb-42f6-8f3c-bf6235312e3d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684d17c-e0e7-43d5-ab8c-673d54473dd7",
   "metadata": {},
   "source": [
    "Давайте также попробуем заглянуть в KNN.\n",
    "\n",
    "Обычно модель KNN с низкими значениями k имеет высокую дисперсию и низкое смещение, но по мере увеличения k дисперсия уменьшается, а смещение увеличивается.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e2ec6-90fe-45e4-ae15-dd19bb97c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "clf_knn.fit(X_train,y_train)\n",
    "y_pred=clf_knn.predict(X_test)\n",
    "\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        clf_knn, X_train, y_train, X_test, y_test,\n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)\n",
    "print('Sklearn 0-1 loss: %.3f' % zero_one_loss(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6737f02-0ec5-450c-b265-ce8aed7cc6bf",
   "metadata": {},
   "source": [
    "Можно заметить, что смещение относительно велико [для k=3] по сравнению с дисперсией. И ожидаемые  ошибки больше, чем у модели RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fa3d5-3b72-452b-8ca1-5eab481796d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [i for i in range(1,21)]\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "val_curve_train, val_curve_test = validation_curve(\n",
    "    estimator=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    param_name=\"n_neighbors\",\n",
    "    param_range=ks,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c8993-2e92-4228-9b51-8ba3d11d2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_err(ks, val_curve_train, label=\"training scores\")\n",
    "plot_with_err(ks, val_curve_test, label=\"validation scores\")\n",
    "plt.xlabel(r\"$\\k$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28613d0-b478-40c3-b7a6-674efcf8fd59",
   "metadata": {},
   "source": [
    "**Задание:** Постройте validation curves для деревьев и случайного леса."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f03e7e-afff-46ad-ad14-6fabecfed661",
   "metadata": {},
   "source": [
    "Для различных значений k в kNN давайте также рассмотрим, какими будут наши ошибки, смещение и дисперсии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99e277-403e-4af2-88d5-5bb4c363b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_KnnClass, var_KnnClass,error_KnnClass, = [], [], []\n",
    "for k in range(1,21):\n",
    "    clf_knn = # your code\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(clf_knn, X_train, y_train, X_test, y_test, loss='0-1_loss', random_seed=123)\n",
    "    bias_KnnClass.append(avg_bias)\n",
    "    var_KnnClass.append(avg_var)\n",
    "    error_KnnClass.append(avg_expected_loss)\n",
    "plt.plot(range(1,21), error_KnnClass, 'red', label = 'total_error',linestyle='dashed')\n",
    "plt.plot(range(1,21), bias_KnnClass, 'brown', label = 'bias^2')\n",
    "plt.plot(range(1,21), var_KnnClass, 'yellow', label = 'variance')\n",
    "plt.xlabel('Algorithm Complexity(K)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490f8e0-fd6b-440a-b7b5-51904155efea",
   "metadata": {},
   "source": [
    "Как и ожидалось, при увеличении k уменьшается дисперсия и немного увеличивается смещение. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0994a-0aff-4f35-98e1-b892229c12d0",
   "metadata": {},
   "source": [
    "**Задание**: Постройте графики зависимости bias-variance от сложности для регрессии\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dac7f7-75a1-4cde-aa37-6610e888ad74",
   "metadata": {},
   "source": [
    "**Задание**: Постройте графики validation_curve от сложности для регрессии. Какая точка, по вашему мнению, соответствует наилучшему набору параметров?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef608b-d7ff-41f2-bc63-e09c64aea038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "students",
   "language": "python",
   "name": "students"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
