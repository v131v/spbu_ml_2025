{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "SEED = 314159\n",
    "TRAIN_TEST_SPLIT = 0.80\n",
    "\n",
    "data_path = r\"C:\\Users\\nikol_ri8fhbe\\Documents\\ml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                           n_redundant=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 2], X[:, 3], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Ансамбли: бустинги\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Бустинг строится последовательно: каждое следующее дерево в нем обучается на основе результатов предыдущего, пытаясь уменьшить его ошибку. Как следствие, композиция будет иметь меньшее смещение, чем исходные базовые модели. Поэтому логично в качестве базовых моделей использовать те, которые изначально обладают небольшим разбросом и высоким смещением. Вопрос: какие это будут модели?\n",
    "Еще одно соображение для выбора сильно смещенных моделей в том, что они банально быстрее учатся. Так как невозможно распараллелить обучение базовых моделей, то скорость их настройки становится серьезным вопросом. \n",
    "\n",
    "Что интересно, бустинги не очень хорошо работают с однородными данными - поэтому их нечасто применяют для текстов.\n",
    "\n",
    "Расссмотрим квадратичную функцию потерь и композицию следующего вида: $ a = b_1 +  b_2 + ... + b_N $\n",
    "Обучим только одно дерево $ a = b_1 $. Найдем примеры, для которых оно ошибается в  предсказании. Обучим для них еще одно дерево - $ b_2 $, которое будет предсказывать ошибку первого. Будем повторять это, пока не наберем K деревьев. Примерно так на верхнем уровне обучается бустинг. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "def eval_classifier(clf):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=43)\n",
    "    n_scores = cross_val_score(clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return np.mean(n_scores), np.std(n_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy = pd.DataFrame(0.0,\n",
    "                                columns=[\"W/o ensembling\", 'Bagging', \"Bagging_with_mf\", 'AdaBoost'],\n",
    "                                index=['deep DTC', '1-level DTC', 'LR', 'SVC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(DecisionTreeClassifier())\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'W/o ensembling'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(DecisionTreeClassifier(max_depth=1))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'W/o ensembling'] = acc_mean\n",
    "print(f\"{acc_mean:.2f} +- {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=1.0, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'Bagging'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=1.0, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'Bagging'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=0.8, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'Bagging_with_mf'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    BaggingClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=10, \n",
    "                      max_samples=1.0, max_features=0.8, \n",
    "                      bootstrap=True, bootstrap_features=False))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'Bagging_with_mf'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Дополните таблицу: обучите также логистическую регрессию с беггингом и без него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost обучает каждый следующий классификатор на объектах, на которых ошибаются предыдущие (объекты с ошибками получают больший вес, без ошибок — меньший)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=50, learning_rate=1.0))\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'AdaBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=1.0))\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'AdaBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "results_to_show = results_accuracy.copy()\n",
    "\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** выясните, дадут ли улучшение бэггинг и бустинг над линейной регрессией. Объясните, почему так."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Почему AdaBoost хуже работает на глубоких деревьях?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Рассмотрим композицию $a = \\sum_{i} {\\gamma_i b_i}$. Для начала выбирается какой-нибудь простой $\\gamma_0, b_0$ (например, 0 и среднее). Формально каждая N-ая модель, начиная со второй, пытается приблизиить антиградиент функционала ошибки, взятый в точках ${z_i=a_{N-1}(x_i)}$:\n",
    "$$s_i = -\\dfrac{\\partial L(y, z)}{\\partial z}|_{z=a_{N-1}}$$\n",
    "Подбор алгоритма при этом производится, приближая эту ошибку c точки зрения квадратичной функции потерь.\n",
    "$$ b_i = arg \\min_{b \\in \\mathcal{B}} {\\sum (b_i(x) - s_i)^2} $$\n",
    "\n",
    "Градиентный бустинг - довольно мощная метамодель, с огромным количеством параметров и хитростей. Мы сегодня остановимся только на основных. Для начала рассмотрим самый стандартный бустинг с использованием деревьев решений (CART). Параметры базовых моделей такие же, как и раньше, но настройка амого бустинга довольно сложна!\n",
    "\n",
    "Важный вопрос при обучении модели - какую функцию ошибок выбрать? Какая задача возникает при обработке датасета с вином?\n",
    "\n",
    "Для того, чтобы оценивать модель, полезны различные метрики - численные характеристики ее качества. При этом бустинги настолько галантны, что предоставляют нам возможность оценивать метрики прямо при обучении. Для этого необходимо задать тип метрики в конструкторе и eval_set при запуске fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "На практике обычно используется один из трех вариантов бустинга - Xgboost, LightGBM или CatBoost.\n",
    "\n",
    "### [XGBoost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf).\n",
    "Плюсы:\n",
    "- Позволяет легко паралелить вычисления (например на спарке)\n",
    "- Легко использовать с sklearn и numpy (но с потерей производительности)\n",
    "- Поддерживается обработка разреженных данных\n",
    "- Предсортированные блоки, кэши, шардирование\n",
    "\n",
    "Минусы:\n",
    "- Нет поддержки GPU\n",
    "\n",
    "[документация](https://xgboost.readthedocs.io/en/latest/)\n",
    "\n",
    "  \n",
    "### [LightGBM](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
    "Плюсы:\n",
    "- Поддержка GPU\n",
    "- Метод Фишера для работы с категориальными признаками\n",
    "- Уменьшение размера обучающей выборки (GOSS)\n",
    "- Объединение разреженных признаков (EFB)\n",
    "\n",
    "Минусы:\n",
    "- Итерфейс не совместим с sklearn/numpy\n",
    "\n",
    "[документация](https://lightgbm.readthedocs.io/en/latest/Python-API.html)\n",
    "\n",
    "### [CatBoost](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
    "Плюсы:\n",
    "- Поддержка GPU\n",
    "- Легко использовать с sklearn и numpy\n",
    "- Более продвинутая работа с категориальными фичами\n",
    "- Наши слоны\n",
    "  \n",
    "Минусы:\n",
    "- Бывает работает хуже (возможно слабее эвристики), но с категориальными фичами — хорошо\n",
    "\n",
    "[документация](https://catboost.ai/docs/concepts/python-quickstart.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "acc_mean, acc_std = eval_classifier(\n",
    "    CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        depth=1,\n",
    "        learning_rate=1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=True, \n",
    "        task_type='CPU'))\n",
    "\n",
    "сat_boost = acc_mean\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'CatBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "acc_mean, acc_std = eval_classifier(\n",
    "    CatBoostClassifier(\n",
    "        iterations=10,\n",
    "        learning_rate=1,\n",
    "        loss_function='Logloss',\n",
    "        verbose=True, \n",
    "        task_type='CPU'))\n",
    "\n",
    "сat_boost = acc_mean\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'CatBoost'] = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "acc_mean, acc_std = eval_classifier(XGBClassifier(objective='binary:logistic', random_state=42))\n",
    "\n",
    "xg_boost = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'XGBoost'] = acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean, acc_std = eval_classifier(XGBClassifier(objective='binary:logistic', max_depth=1, random_state=42))\n",
    "\n",
    "xg_boost = acc_mean\n",
    "print(f\"{acc_mean:.2f}, {acc_std:.2f}\")\n",
    "\n",
    "results_accuracy.loc['1-level DTC', 'XGBoost'] = acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_show = results_accuracy.copy()\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "param = {'num_leaves': 31, \n",
    "         'objective': 'multiclass', \n",
    "         'num_class': 2, \n",
    "         'metric': ['multi_logloss']}\n",
    "\n",
    "num_round = 10\n",
    "boost = lgb.train(param, train_data, num_boost_round=10)\n",
    "\n",
    "lg_boost = (boost.predict(X).argmax(axis=-1) == y).mean()\n",
    "\n",
    "results_accuracy.loc['deep DTC', 'LightGBM'] = lg_boost\n",
    "print(f\"{lg_boost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_show = results_accuracy.copy()\n",
    "\n",
    "results_to_show.style.apply(highlight_max, props='font-weight: bold;', axis=1).format('{:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример\n",
    "Рассмотрим реальный датасет, и на его примере попробуем поработать с бустингом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_diabetes()\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.5, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(n_estimators=100, learning_rate=1, seed=SEED)\n",
    "fit_params = {\"eval_set\":[(X_train, y_train),(X_test, y_test)], \"verbose\": False}\n",
    "# Add verbose=False to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = model.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "error_function = \"rmse\"\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][error_function], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][error_function], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Как мы видим, хотя лосс при обучении падал и падал, на валидации метрики перестали улучшаться довольно рано. Это довольно плохой знак. Однако говорит ли это о катастрофической ситуации? Проверим переобучение с помощью кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, Y, cv=10, scoring=[\"neg_root_mean_squared_error\"],\n",
    "                            return_train_score=True)\n",
    "print(\"Train RMSE is\", -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "print(\"Test RMSE is\", -cv_results['test_neg_root_mean_squared_error'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Кажется, у нас действительно серьезные проблемы. Попробуем уменьшить скорость обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train and eval model with smaller lr\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.01, seed=SEED)\n",
    "fit_params = {\"eval_set\":[(X_train, y_train),(X_test, y_test)], \"verbose\": False}\n",
    "# Add verbose=False to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "results = model.evals_result()\n",
    "error_function = \"rmse\"\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][error_function], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][error_function], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Помогло ли это? Попробуем получить результаты лучше, поиграв с параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, Y, cv=10, scoring=[\"neg_root_mean_squared_error\"],\n",
    "                            return_train_score=True)\n",
    "print(\"Train RMSE is\", -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "print(\"Test RMSE is\", -cv_results['test_neg_root_mean_squared_error'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(\n",
    "    # your params\n",
    "\n",
    ")\n",
    "\n",
    "# train, test and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Так как параметров довольно много, может быть разумно автоматизировать их поиск. Для этого воспользуемся поиском по решетке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgboost_params = {\n",
    "    # set your params range\n",
    "                 }\n",
    "xgboost_best_grid = GridSearchCV(model, xgboost_params,\n",
    "                                 cv=7, n_jobs=-1,\n",
    "                                 return_train_score=True).fit(X_train, y_train,**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(xgboost_best_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Давайте проверим, какую точность мы получим с лучшими параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train and test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** Проведите обучение и с LightGBM/CatBoost. Какие лучшие точности у вас получилось получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Задание:** Постройте графики предсказаний для первых двух PCA фичей для бустингов разной глубины/разного числа деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Постройте график зависимости точности от глубины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
